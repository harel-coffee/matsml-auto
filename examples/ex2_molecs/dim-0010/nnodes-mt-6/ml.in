# Type of run
run_type         = learning_curve   # Options are "learning_curve", "make_model"
number_runs      = 1               # Number of runs to make the learning curve

# Input
file_data        = /expanse/lustre/scratch/huantran/temp_project/perov/data/HOIP_comp_RFPE_Ymean_afp_wstd_-0010.csv
id_col           = ID
y_cols           = Ymean Ystd
#comment_cols     = onehot1 onehot2 prop_value          # comment columns
y_scale          = minmax          # none, minmax, normal
x_scale          = minmax       # log, normalize, minmax, quantile, yeo-johnson

# Output
file_model       = model.pkl        # Model saved for further use

# ML algo & package
ml_algo          = nn tensor-flow   # Algo & package, current options are "krr-cv sk-learn", "nn tensor-flow", 
                                    # "gpr-cv sk-learn", "fnn deepdl". For tensor-flow, keras API is used.
                                    #
sampling         = random           # Sampling methods, curent options are "pca-grid" and "random", more to come. 
                                    # "pca-grid" is a specific stratified sampling methods based on a PCA of the data.
nfold-cv         = 5                # Number of folds in cross validation. Currently, cross validation is 
                                    # required for all the trainings
model_perf       = best_test         # How to select better model, options are "balance", "best_test"

# For "nn tensor-flow" only
nlayers          = 2                # Number of hidden layers, not including the input and output layers
                                    #
nneurons         = 6                # Number of neurons per layer. Assumed the same for all layers(but, how can we 
                                    # assume others?)
activ_func       = tanh             # Activation function, assumed the same for all layers. Options are "tanh", 
                                    # "relu", "sigmoid", "softmax", "softplus", "softsign", "selu", "elu", "exponential"  
epochs           = 15000          # Training parameters, numer of backpropagations
                                    #
batch_size       = 32              # Batch size, default = 32

optimizer        = nadam            # Optimizier, options are "SGD", "RMSprop", "Adam", "Adadelta", "Adagrad", "Adamax", 
                                    # "Nadam", "Ftrl"
use_bias         = True             # Use bias or not, options are "True" and "False"

x_in             = x_in.csv         
fbasis_dim       = 64               # Dimensionality of the function basis set. Currently uses set of equispaced Gaussian
xstart           = 0
xend             = 5
decom_train      = False


# Train and test data options, for learning_curve only
ml_ntrains_start = 176              # The starting point to make learning curve
ml_ntrains_stop  = 193             # The ending point of learning curve
ml_ntrains_incre = 1600              # Increasement of points in learning curve

